{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aman-17/actualwise/blob/main/simplified_chunking_actualwise_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txq3SX7KF981"
      },
      "outputs": [],
      "source": [
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract pdf2image\n",
        "!apt-get install poppler-utils\n",
        "!pip install --upgrade git+https://github.com/huggingface/transformers\n",
        "!pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcLFkRx0IW3Z"
      },
      "outputs": [],
      "source": [
        "!mkdir 'images'\n",
        "!mkdir 'chunk_deid1'\n",
        "!mkdir 'chunk1'\n",
        "!mkdir 'chunk2'\n",
        "!mkdir 'chunk1_outputs'\n",
        "!mkdir 'chunk2_outputs'\n",
        "!mkdir 'merged_deid1'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import subprocess\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Specify the folders\n",
        "pdf_folder = '/content/pdf'\n",
        "image_output_folder = '/content/image/'\n",
        "text_output_folder = '/content/text/'\n",
        "\n",
        "# Ensure the output folders exist\n",
        "os.makedirs(image_output_folder, exist_ok=True)\n",
        "os.makedirs(text_output_folder, exist_ok=True)\n",
        "\n",
        "oem_val = 3  # Example OEM value\n",
        "psm_val = 6  # Example PSM value\n",
        "\n",
        "def clean_embedded_page_numbers(text):\n",
        "    pattern = r'Page\\s+\\d+(\\s*/\\s*\\d+)*(\\s+of\\s+\\d+)?'\n",
        "    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "    pattern_no_spaces = r'Page\\d+of\\d+'\n",
        "    cleaned_text = re.sub(pattern_no_spaces, '', cleaned_text, flags=re.IGNORECASE)\n",
        "    return cleaned_text\n",
        "\n",
        "def ocr_image(args):\n",
        "    image_path, config, image_output_path = args\n",
        "    # Constructing the Tesseract command\n",
        "    command = ['tesseract', image_path, image_output_path, config]\n",
        "    # Running the Tesseract command\n",
        "    subprocess.run(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "    # Reading the output text from file\n",
        "    with open(f\"{image_output_path}.txt\", 'r') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "# Process each PDF file\n",
        "for pdf_file in os.listdir(pdf_folder):\n",
        "    if pdf_file.endswith(\".pdf\"):\n",
        "        start_time_pdf = time.time()\n",
        "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
        "        print(f\"Processing PDF: {pdf_path}\")\n",
        "\n",
        "        pdf_base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "        text_output_file_name = f\"{pdf_base_name}_oem{oem_val}_psm{psm_val}_600_dpi.txt\"\n",
        "        text_output_file_path = os.path.join(text_output_folder, text_output_file_name)\n",
        "\n",
        "        images = convert_from_path(pdf_path, 600)\n",
        "\n",
        "        # Preparing arguments for parallel processing\n",
        "        ocr_args = []\n",
        "        for i, image in enumerate(images):\n",
        "            image_file_path = os.path.join(image_output_folder, f\"{pdf_base_name}_page_{str(i+1).zfill(2)}.png\")\n",
        "            image.save(image_file_path)\n",
        "            config = f'--oem {oem_val} --psm {psm_val}'\n",
        "            image_output_path = os.path.join(image_output_folder, f\"{pdf_base_name}_page_{str(i+1).zfill(2)}\")\n",
        "            ocr_args.append((image_file_path, config, image_output_path))\n",
        "\n",
        "        # Using multiprocessing to process images in parallel\n",
        "        with Pool() as pool:\n",
        "            texts = pool.map(ocr_image, ocr_args)\n",
        "\n",
        "        with open(text_output_file_path, 'w', encoding='utf-8') as text_output_file:\n",
        "            for i, text in enumerate(texts):\n",
        "                cleaned_text = clean_embedded_page_numbers(text)\n",
        "                text_output_file.write(f'PDF Page Number {i + 1}\\n{cleaned_text}\\n\\n' + '-'*60 + '\\n')\n",
        "\n",
        "        print(f\"OCR text for {pdf_base_name} saved to {text_output_file_path} in {time.time() - start_time_pdf:.2f} seconds.\")\n",
        "\n",
        "#"
      ],
      "metadata": {
        "id": "mzpoyf0r7lLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUCru6u90N6F",
        "outputId": "e4bde940-c3cd-46e0-bafe-f0dc02063e7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OCR text saved to /content/images/CaseStudy_3_ZB_oem3_psm6_pytesseract_poppler_dpi600.txt\n"
          ]
        }
      ],
      "source": [
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import re\n",
        "import os\n",
        "\n",
        "\n",
        "oem_val = 3  # Example OEM value\n",
        "psm_val = 6  # Example PSM value - 3, 4, 6? 1, 5, 7, 11, 12 (6 is best for Case 1)\n",
        "custom_config = f'--oem {oem_val} --psm {psm_val}'\n",
        "\n",
        "pdf_path = '/content/CaseStudy_3_ZB.pdf'\n",
        "\n",
        "output_folder = '/content/images'\n",
        "pdf_base_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "\n",
        "output_file_name = f\"{pdf_base_name}_oem{oem_val}_psm{psm_val}_pytesseract_poppler_dpi600.txt\"\n",
        "output_file_path = os.path.join(output_folder, output_file_name)\n",
        "\n",
        "def clean_embedded_page_numbers(text):\n",
        "    # Matches 'Page X', 'Page X/Y/Z', and 'Page X of Y' (with variations in spacing and 'of' usage)\n",
        "    pattern = r'Page\\s+\\d+(\\s*/\\s*\\d+)*(\\s+of\\s+\\d+)?'\n",
        "    cleaned_text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Matches 'PageXofY' without spaces\n",
        "    pattern_no_spaces = r'Page\\d+of\\d+'\n",
        "    cleaned_text = re.sub(pattern_no_spaces, '', cleaned_text, flags=re.IGNORECASE)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "images = convert_from_path(pdf_path, 600)\n",
        "\n",
        "with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
        "    for i, image in enumerate(images):\n",
        "        text = pytesseract.image_to_string(image, config=custom_config)\n",
        "        cleaned_text = clean_embedded_page_numbers(text)\n",
        "        # Inserting PDF page number\n",
        "        output_file.write(f'PDF Page Number {i + 1}\\n{cleaned_text}\\n\\n' + '-'*60 + '\\n')\n",
        "\n",
        "print(f\"OCR text saved to {output_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# List of insurance companies\n",
        "insurance_companies = [\n",
        "    \"Blue Cross and Blue Shield of Illinois\", \"BCBSIL\", \"Health Care Service Corporation\", \"HCSC\",\n",
        "    \"Aetna\",\n",
        "    \"UnitedHealthcare\", \"UHC\", \"UHG\",\n",
        "    \"Cigna\",\n",
        "    \"Humana\",\n",
        "    \"Health Alliance\",\n",
        "    \"Molina Health care\", \"Molina\",\n",
        "    \"Meridian Health\", \"Meridian\",\n",
        "    \"Medicaid\",\n",
        "    \"Medicare\",\n",
        "    \"insurance\"\n",
        "]\n",
        "\n",
        "# Open and read the file\n",
        "with open('/content/images/CaseStudy_3_ZB_oem3_psm6_pytesseract_poppler_dpi600.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Search for the insurance companies in the file\n",
        "for company in insurance_companies:\n",
        "    if re.search(company, data, re.IGNORECASE):\n",
        "        print(f\"The patient is insured with {company}\")\n"
      ],
      "metadata": {
        "id": "sj_4FwgWLWXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c668660e-d1b6-4a2f-d981-a5171d8382e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The patient is insured with Meridian Health\n",
            "The patient is insured with Meridian\n",
            "The patient is insured with Medicaid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NKHSDYjy4od"
      },
      "outputs": [],
      "source": [
        "# Simplified chunking\n",
        "import os\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Setup\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
        "print(\"Tokenizer loaded.\")\n",
        "\n",
        "def tokenize_text_continuously(text, max_length):\n",
        "    print(\"Tokenizing text and creating continuous chunks...\")\n",
        "    token_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    chunks = []\n",
        "    token_start = 0\n",
        "    while token_start < len(token_ids):\n",
        "        token_end = min(token_start + max_length, len(token_ids))\n",
        "        chunk_token_ids = token_ids[token_start:token_end]\n",
        "        chunk = tokenizer.decode(chunk_token_ids, skip_special_tokens=True)\n",
        "        chunks.append((chunk, token_start, token_end-1))\n",
        "        token_start = token_end\n",
        "    return chunks\n",
        "\n",
        "def chunk_files(input_directory, output_directory, max_length=512):\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    print(f\"Processing files from {input_directory} to save chunks in {output_directory}\")\n",
        "\n",
        "    for filename in os.listdir(input_directory):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            print(f\"Processing file: {filename}\")\n",
        "            with open(os.path.join(input_directory, filename), 'r') as file:\n",
        "                text = file.read()\n",
        "\n",
        "            first_pass_chunks = tokenize_text_continuously(text, max_length)\n",
        "            total_chunks = len(first_pass_chunks)\n",
        "            max_digits_chunk = len(str(total_chunks))\n",
        "\n",
        "            for i, (chunk, start_token, end_token) in enumerate(first_pass_chunks, start=1):\n",
        "                chunk_num_padded = str(i).zfill(max_digits_chunk)\n",
        "                chunk_filename = f\"Chunk{chunk_num_padded}_Tokens{start_token}-{end_token}.txt\"\n",
        "                with open(os.path.join(output_directory, chunk_filename), 'w') as chunk_file:\n",
        "                    chunk_file.write(chunk)\n",
        "                    print(f\"Saved chunk {chunk_filename}.\")\n",
        "\n",
        "# Example usage\n",
        "input_directory = \"/content/images/\"\n",
        "output_directory = \"/content/chunks/\"\n",
        "chunk_files(input_directory, output_directory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the tokenizer and model from the pretrained \"obi/deid_roberta_i2b2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"obi/deid_roberta_i2b2\").to(device)\n",
        "nlp = pipeline(task=\"ner\", model=\"obi/deid_roberta_i2b2\",device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "\"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpie/clinical-distilbert-i2b2-2010\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"nlpie/clinical-distilbert-i2b2-2010\").to(device)\n",
        "nlp=pipeline(task=\"ner\", model=\"obi/deid_bert_i2b2\",device=0)\n",
        "\"\"\"\n",
        "\n",
        "def extract_info(text, ner_results):\n",
        "    # Sort the entities based on their start position\n",
        "    sorted_entities = sorted(ner_results, key=lambda x: x['start'])\n",
        "    merged_entities = []\n",
        "\n",
        "    # Merge overlapping entities\n",
        "    for entity in sorted_entities:\n",
        "        if merged_entities and entity['start'] <= merged_entities[-1]['end']:\n",
        "            merged_entities[-1]['end'] = max(merged_entities[-1]['end'], entity['end'])\n",
        "            merged_entities[-1]['entity'] = entity['entity']\n",
        "        else:\n",
        "            merged_entities.append(entity)\n",
        "\n",
        "    # Extract the identified entities in the text\n",
        "    extracted_info = {}\n",
        "    for entity in merged_entities:\n",
        "        if entity['entity'] in ['age', 'gender', 'name']:\n",
        "            extracted_info[entity['entity']] = text[entity['start']:entity['end']]\n",
        "\n",
        "    return extracted_info\n",
        "\n",
        "# Define the directory containing the text files\n",
        "directory = \"/content/chunks/\"\n",
        "# Define the directory to save the deidentified text files\n",
        "new_directory = \"/content/chunk_deid1\"\n",
        "\n",
        "# Loop through the text files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Open the text file and read the text\n",
        "        with open(os.path.join(directory, filename), 'r') as file:\n",
        "            text = file.read()\n",
        "            # Perform NER on the text\n",
        "            ner_results = nlp(text)\n",
        "            # Deidentify the text\n",
        "            deidentified_text = extract_info(text, ner_results)\n",
        "            print(deidentified_text)\n",
        "        # Write the deidentified text to a new file\n",
        "        # with open(os.path.join(new_directory, \"deidentified_\" + filename), 'w') as file:\n",
        "            # file.write(deidentified_text)"
      ],
      "metadata": {
        "id": "TAGJve2hNle5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203,
          "referenced_widgets": [
            "93450af3eaff4829b6007c37dd9432d0",
            "34c289e56b52439a936077023f137a88",
            "c5a8487a2c4641b7bf95befc59c5f7e5",
            "2b5a3ecc7a2c4256890e4ba5f7ed762c",
            "963c0544c5204778aa988c53ca2cb062",
            "911b69f5ad2b4e89a18c0db59083c120",
            "b55747b61eba49ff96e777f9428f755d",
            "683de6500e604034960a10e88347e7f8",
            "1c429dfb87954a7fa913b8abcca8e01a",
            "716dc9a4a9c6407cab1f9df3a80345cc",
            "c8a28669b59b449491c7001f862b04d8",
            "642d2130631348a4ac0a23c7593c5f30",
            "f37c628088014aecb0cd21ca8db97539",
            "128f3645ed094306a8d9b06f0e26316a",
            "afa8c7f8772c4be2bc8127502c639016",
            "304f3d1be9e34a76b1c1f00d7c303b24",
            "4fcc8aae02fc47e4bed197fdd88e77b1",
            "d006fa6d3e074ef2a4caaeb6d44c1f0d",
            "34b6f0823ed84351b67ea9652c3065c8",
            "96b04859711847d09293bae57a5ee77a",
            "2f0b5807e6e84f8886c0a7cc61a71a1b",
            "f8ae1995a01646998c88de0795f53ae5"
          ]
        },
        "id": "ZAYvFUUF0c5x",
        "outputId": "9b7cea96-b37b-46f7-c024-ccae4438bef1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93450af3eaff4829b6007c37dd9432d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "642d2130631348a4ac0a23c7593c5f30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at obi/deid_roberta_i2b2 and are newly initialized: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the tokenizer and model from the pretrained \"obi/deid_roberta_i2b2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"obi/deid_roberta_i2b2\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"obi/deid_roberta_i2b2\").to(device)\n",
        "nlp = pipeline(task=\"ner\", model=\"obi/deid_roberta_i2b2\",device=0 if torch.cuda.is_available() else -1)\n",
        "\n",
        "\"\"\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"nlpie/clinical-distilbert-i2b2-2010\")\n",
        "model = AutoModelForMaskedLM.from_pretrained(\"nlpie/clinical-distilbert-i2b2-2010\").to(device)\n",
        "nlp=pipeline(task=\"ner\", model=\"obi/deid_bert_i2b2\",device=0)\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to deidentify text based on the NER results\n",
        "def deidentify_text(text, ner_results):\n",
        "    # Sort the entities based on their start position\n",
        "    sorted_entities = sorted(ner_results, key=lambda x: x['start'])\n",
        "    merged_entities = []\n",
        "\n",
        "    # Merge overlapping entities\n",
        "    for entity in sorted_entities:\n",
        "        if merged_entities and entity['start'] <= merged_entities[-1]['end']:\n",
        "            merged_entities[-1]['end'] = max(merged_entities[-1]['end'], entity['end'])\n",
        "            merged_entities[-1]['entity'] = entity['entity']\n",
        "        else:\n",
        "            merged_entities.append(entity)\n",
        "\n",
        "    # Replace the identified entities in the text with their entity type\n",
        "    deidentified_text = text\n",
        "    for entity in reversed(merged_entities):\n",
        "        start = entity['start']\n",
        "        end = entity['end']\n",
        "        deidentified_text = deidentified_text[:start] + \"[\" + entity['entity'] + \"]\" + deidentified_text[end:]\n",
        "    return deidentified_text\n",
        "\n",
        "# Define the directory containing the text files\n",
        "directory = \"/content/chunks/\"\n",
        "# Define the directory to save the deidentified text files\n",
        "new_directory = \"/content/chunk_deid1\"\n",
        "\n",
        "# Loop through the text files in the directory\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Open the text file and read the text\n",
        "        with open(os.path.join(directory, filename), 'r') as file:\n",
        "            text = file.read()\n",
        "            # Perform NER on the text\n",
        "            ner_results = nlp(text)\n",
        "            # Deidentify the text\n",
        "            deidentified_text = deidentify_text(text, ner_results)\n",
        "        # Write the deidentified text to a new file\n",
        "        with open(os.path.join(new_directory, \"deidentified_\" + filename), 'w') as file:\n",
        "            file.write(deidentified_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CesPDPZUbQe9",
        "outputId": "755c0297-14d0-4ec8-ab33-8a8e55c1663a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All files have been successfully merged into /content/merged_deid1/merged_deidentified_text.txt\n"
          ]
        }
      ],
      "source": [
        "# Merge deid files\n",
        "import os\n",
        "\n",
        "# Define the directory containing the deidentified chunk files\n",
        "input_directory = \"/content/chunk_deid1/\"\n",
        "# Define the path for the output file\n",
        "output_file_path = \"/content/merged_deid1/merged_deidentified_text.txt\"\n",
        "\n",
        "# Get all filenames in the directory\n",
        "filenames = os.listdir(input_directory)\n",
        "\n",
        "# Sort the filenames simply by their natural order; this assumes the names are structured for correct sorting\n",
        "sorted_filenames = sorted(filenames)\n",
        "\n",
        "# Check if there are files to process\n",
        "if not sorted_filenames:\n",
        "    print(\"No files found in the directory.\")\n",
        "else:\n",
        "    # Open the output file in write mode\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        # Iterate over each sorted filename\n",
        "        for filename in sorted_filenames:\n",
        "            # Print the filename being processed\n",
        "            # print(f\"Including file in merge: {filename}\")\n",
        "\n",
        "            # Construct the full path to the file\n",
        "            file_path = os.path.join(input_directory, filename)\n",
        "            # Open and read the content of the file\n",
        "            with open(file_path, 'r') as file:\n",
        "                content = file.read()\n",
        "                # Append the content to the output file\n",
        "                output_file.write(content + \"\\n\")  # Add a newline to separate each file's content\n",
        "\n",
        "    print(f\"All files have been successfully merged into {output_file_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMzn1cOZT__C"
      },
      "outputs": [],
      "source": [
        "# 2 Chunks for GPT_3.5 model\n",
        "def split_file(filename):\n",
        "    with open(filename, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    half = len(lines) // 2\n",
        "    with open('chunked_deid_file1.txt', 'w') as first_half_file:\n",
        "        first_half_file.writelines(lines[:half])\n",
        "\n",
        "    with open('chunked_deid_file2.txt', 'w') as second_half_file:\n",
        "        second_half_file.writelines(lines[half:])\n",
        "\n",
        "split_file('/content/merged_deid1/merged_deidentified_text.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5B0h4SnXnGn",
        "outputId": "87ff55ad-52e1-433a-9e9f-504071ad119a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CRE: No\n",
            "C-Auris: No\n",
            "MRSA: No\n",
            "C-DIFF: No\n",
            "MDRO: No\n",
            "COVID: No\n",
            "A&Ox4 - Pages 11, 12\n",
            "COVID vaccination: No\n",
            "COVID test: NA.\n",
            "The patient has a short-term care plan. Here is the plan listed in bullet format along with the page number(s) as the source:\n",
            "\n",
            "- Inpatient Medication Plan: \n",
            "  1. Acetaminophen 325 mg tablet every 4 hours PRN for pain\n",
            "  2. Lorazepam 1 mg tablet every 6 hours PRN for anxiety\n",
            "  3. Dicyclomine 20 mg tablet three times daily for stomach cramps\n",
            "  4. Gabapentin 600 mg tablet three times daily for anxiety\n",
            "  5. Loperamide 2 mg capsule every 2 hours PRN for diarrhea (not more than 16 mg per day)\n",
            "  6. Lorazepam 2 mg IM every 4 hours PRN for seizures only\n",
            "\n",
            "Expected duration is not specified in the provided information.\n",
            "\n",
            "Source: Pages 13, [U-ID]\n",
            "No long-term care plan mentioned.\n",
            "- Major depressive disorder\n",
            "- Polysubstance abuse\n",
            "- Seizures related to alcohol withdrawal\n",
            "- Left lower extremity deep vein thrombosis (DVT) and pulmonary embolism (PE) treated with Xarelto \n",
            "- Gastroesophageal reflux disease (GERD) managed with omeprazole\n",
            "- Alcoholic hepatitis\n",
            "- Avascular necrosis in the left hip leading to pain, managed with tramadol\n",
            "- Nicotine dependence with the use of a nicotine patch\n",
            "- Diagnoses: NA\n",
            "\n",
            "Source: Pages 1-13\n",
            "- LLE DVT, PE (years unknown)\n",
            "- GERD (years unknown)\n",
            "- Congenital spondylosethesis (years unknown)\n",
            "- Left hip avascular necrosis (years unknown)\n",
            "- History of hip replacement (years unknown)\n",
            "- History of back surgery (years unknown)\n",
            "- History of seizures from withdrawal (years unknown)\n",
            "- Current hospitalization reason: Major depression, polysubstance abuse, seizure-di/t alcohol withdrawal, alcoholic hepatitis, left hip avascular necrosis/pain, nicotine dependence (Page 6, Page 13)\n",
            "- Elapsed time since each historical event or diagnosis is not specified in the provided medical records.\n",
            "Mobility Aids: Wheelchair - Page 6, Rolling Walker - Page 6\n",
            "Patient Monitoring & Safety Equipment:\n",
            "\n",
            "- Vital signs monitor for regular blood pressure, heart rate, temperature, and respiratory rate monitoring (Page 8)\n",
            "- Pulse oximeter for monitoring oxygen saturation levels (Page 8)\n",
            "- Continuous monitoring device for assessing SPO2 levels (Page 8)\n",
            "- Monitoring equipment for conducting neurological assessments including cranial nerves (Page 12)\n",
            "- Equipment for assessing level of consciousness and mental status (Page 11)\n",
            "- Equipment for performing ongoing seizure activity assessment (Page 13)\n",
            "- Bed Equipment: NA\n",
            "Specialized Medical Equipment: \n",
            "- Glucose monitoring device\n",
            "- Oxygen equipment for alcoholic hepatitis\n",
            "- CPAP or BiPAP machine for possible respiratory issues\n",
            "- Seizure monitoring equipment for seizure-di/t alcohol withdrawal\n",
            "- Nebulizer for potential respiratory concerns\n",
            "- Tracheostomy equipment for possible complications related to major depression\n",
            "- IV or Infusion Pumps for medication administration\n",
            "- Specialized wound care equipment for LLE DVT/PE-Xarelto\n",
            "Source: PDF Page 13\n",
            "- Assistance: Minimal Assistance, One-on-one (1:1) Feeding Assistance\n",
            "- Source: Page 13\n",
            "IV: NA. Page 13\n",
            "Pressure ulcers: NA\n",
            "Page 13\n",
            "Complex wounds: No. Source: Page [U-ID].\n",
            "\"Wound Vac: No\" - There is no indication in the provided medical records that the patient needs a wound vacuum. The information was gathered from Page 1 to Page [U-ID].\n",
            "Specialty Wound Equipment: No. [Page numbers not provided for this response]\n",
            "Yes, the patient is a previous smoker with a history of tobacco use at 1 pack per day.  \n",
            "Source: Page 9\n",
            "• Cannabis: Smoke/Vape sporadically, started in the patient's teens, last used recently (Page 9)\n",
            "• Cocaine/Crack: Snort/Inhale, $40 daily usage, started in the patient's 20s, last used recently (Page 9)\n",
            "• Opiates (Heroin/Oxy): Inject, $80 daily usage, started at age 18, last used recently (Page 9)\n",
            "• Yes, the patient has a history of alcohol abuse. \n",
            "• Substances: Alcohol, daily heavy use \n",
            "• Frequency: Daily \n",
            "• Source: Medical History and Physical Examination (Pages 8, 9)\n",
            "Therapy: NA  \n",
            "(Page 1, Page 2, Page 3, Page 4, Page 5, Page 6, Page 7, Page 8, Page 9, Page 10, Page 11, Page 12, Page 13)\n",
            "Concerns: NA\n",
            "\n",
            "Page 13\n",
            "Tuberculosis: No.\tPage 8\n",
            "VRE: No. Page 12\n",
            "HIV: No. Source: Pages 1-13.\n",
            "ESBL: No. Source: Pages 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n",
            "AIDS: No. Page 11\n",
            "ECG: Yes - Page [U-ID]\n",
            "PT/INR: Yes. Pages 6, 12\n",
            "eCardio: No. Pages 13.\n",
            "Bladder scan: No. Page 13\n",
            "Central line: No. Page 1, Page 2\n",
            "Midline catheter: No. Source: Page 13.\n",
            "Implantable ports: No. Page 13.\n",
            "JP/Penrose drain: No. Pages 1-13.\n",
            "Nephrostomy: No. Source: Page 1-13.\n",
            "Colostomy: No. Source: Page 6.\n",
            "Suprapubic Catheter: No. Page 1, Page 7.\n",
            "Dialysis Shunt: No. Source - Page 13.\n",
            "Dialysis Catheter: No. Source: Page 6, Page 7.\n",
            "BiPAP/CPAP: No. Page 13.\n",
            "Tracheostomy: No\n",
            "Oxygen: No. [Page 13]\n",
            "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
            "None\n",
            "Chest tube: No. Pages 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13\n",
            "Pleurex: No. Page 1, Page 2, Page 3, Page 4, Page 5, Page 6, Page 7, Page 8, Page 9, Page 10, Page 11, Page 12\n",
            "Surgical Orthopedic care: Yes. Source: Page 6.\n",
            "External fixator: No. Page 13.\n",
            "Orthopedic Traction: No. Page [U-ID].\n",
            "Prosthetics: No - Pages [U-ID]\n",
            "CABG: No. Source: Page 6.\n",
            "LVAD: No - Pages 6, 7, 8, 9\n",
            "LifeVest: No. Page 1\n",
            "Pacemaker: No. Source: Pages 1-13.\n",
            "Hemodialysis: No. Page 13.\n",
            "Peritoneal dialysis: No. Pages 1, 2, 4, 6, 8, 9, 10.\n",
            "Bariatric: No. Source: Pages 6, 8, 9\n",
            "IV: No. Page 13\n",
            "TPN: No. Pages 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13.\n",
            "PCA: No. Source: Page 13.\n",
            "IV Push: Yes. Page 13\n",
            "Oral Chemotherapy: No. Pages 11\n",
            "IV Chemo: No. Source: Page 13.\n",
            "High Cost Rx: Yes. Pages 7, 9, 13.\n",
            "Psychosocial Care: Yes - Pages 8, 9\n",
            "Dementia: No. Pages 6-13\n",
            "Recent danger: Yes. Source: Pages 6, 7, 8, 9, 10.\n",
            "Restraints: No. Page 13\n",
            "Closed Unit: Yes. Pages 11, 12, 13.\n",
            "Felon: Yes - Pages 10, 12.\n",
            "Sexual Offender: No. (Source: Page 9)\n",
            "Etoh/Alcohol-current: Yes - Page 8\n",
            "Etoh/Alcohol-history: Yes (Pages 6, 8, 13)\n",
            "Drugs-current: Yes. Page 9\n",
            "Drugs-history: Yes. Pages 5, 6, 9\n",
            "Head Injury: No. Source Pages: 1, 6, 11\n",
            "Less<18 yrs: No. Pages 6, 7, 8, 9, 10, 11, 12, 13\n",
            "Medicare/Medicaid/NA\n"
          ]
        }
      ],
      "source": [
        "# !pip install openai==0.28\n",
        "\n",
        "import pandas as pd\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "import time\n",
        "\n",
        "# Ensure your OpenAI API key is correctly set here or through environment variables\n",
        "openai.api_key = userdata.get('GPT_API')\n",
        "\n",
        "# Read the file content\n",
        "with open('/content/chunked_deid_file1.txt', 'r') as file:\n",
        "    content1 = file.read()\n",
        "\n",
        "df = pd.read_excel('./ChatGPT prompts v5.xlsx', sheet_name=1)\n",
        "questions = df['Rafi\\'s Qs'].tolist()\n",
        "\n",
        "def get_gpt3_5_turbo_response(prompt):\n",
        "    try:\n",
        "        # Construct the messages parameter correctly as a list of dictionaries\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful medical record retriever.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0125\",\n",
        "            messages=messages,\n",
        "            # max_tokens=16000,  # Adjust based on the expected length of completion, ensuring total does not exceed 16385\n",
        "            # temperature=1,\n",
        "            # top_p=1,\n",
        "            # frequency_penalty=0,\n",
        "            # presence_penalty=0\n",
        "        )\n",
        "\n",
        "        # Assuming the response structure matches expected output\n",
        "        return response.choices[0].message['content'].strip() if response.choices else \"No response\"\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "with open('chunk1_answers.txt', 'w') as f:\n",
        "    for question in questions:\n",
        "        prompt = content1 + '\\n Based on the above patient\\'s medical record. Answer the following question. \\n' + question\n",
        "        response_text = get_gpt3_5_turbo_response(prompt)\n",
        "        print(response_text)\n",
        "        f.write(f\"Question:\\n{question}\\nAnswer:\\n{response_text}\\n{'-'*32}\\n\")\n",
        "        time.sleep(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Ensure your OpenAI API key is correctly set here or through environment variables\n",
        "openai.api_key = userdata.get('GPT_API')\n",
        "\n",
        "# Read the file content\n",
        "with open('/content/chunked_deid_file2.txt', 'r') as file:\n",
        "    content2 = file.read()\n",
        "\n",
        "df = pd.read_excel('./ChatGPT prompts v5.xlsx', sheet_name=1)\n",
        "questions = df['Rafi\\'s Qs'].tolist()\n",
        "\n",
        "def get_gpt3_5_turbo_response(prompt):\n",
        "    try:\n",
        "        # Construct the messages parameter correctly as a list of dictionaries\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful medical record retriever.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0125\",\n",
        "            messages=messages,\n",
        "        )\n",
        "\n",
        "        # Assuming the response structure matches expected output\n",
        "        return response.choices[0].message['content'].strip() if response.choices else \"No response\"\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "with open('chunk2_answers.txt', 'w') as f:\n",
        "    for question in questions:\n",
        "        prompt = content2 + '\\n Based on the above patient\\'s medical record. Answer the following question. \\n' + question\n",
        "        response_text = get_gpt3_5_turbo_response(prompt)\n",
        "        f.write(f\"Question:\\n{question}\\nAnswer:\\n{response_text}\\n{'-'*32}\\n\")\n"
      ],
      "metadata": {
        "id": "bQ6OoVbHJ52X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nefvY3kYf9P"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import openai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Ensure your OpenAI API key is correctly set here or through environment variables\n",
        "openai.api_key = userdata.get('GPT_API')\n",
        "\n",
        "with open('/content/chunk1_answers.txt', 'r') as file:\n",
        "    content1 = file.read()\n",
        "\n",
        "with open('/content/chunk2_answers.txt', 'r') as file:\n",
        "    content2 = file.read()\n",
        "\n",
        "def get_gpt3_5_turbo_response(prompt):\n",
        "    try:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful medical record retriever.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ]\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0125\",\n",
        "            messages=messages,\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message['content'].strip() if response.choices else \"No response\"\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "with open('final_answers.txt', 'w') as f:\n",
        "    prompt = content1 + '\\n\\n' + content2 + '\\n Summarize these statements, grouping similar items together, but include all page number references. Where statements include both \\'Yes\\' and \\'No\\', keep the \\'Yes\\' response with its page number references and omit the \\'No\\' statements. The goal is to aggregate the existing text, keeping it as concise as possible without adding any new text.\"'\n",
        "    response_text = get_gpt3_5_turbo_response(prompt)\n",
        "    f.write(f\"{response_text}\\n{'-'*32}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL2KlNuxx7lr",
        "outputId": "5434fb91-fadc-4625-fc18-7ebf77a69f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. CRE: No\n",
            "2. C-Auris: No\n",
            "3. MRSA: No\n",
            "4. C-DIFF: No\n",
            "5. MDRO: No\n",
            "6. COVID: No\n",
            "7. A&OxNA\n",
            "8. COVID vaccination: NA\n",
            "9. COVID test: NA\n",
            "10. No short-term care plan mentioned\n",
            "11. No long-term care plan mentioned\n",
            "12. Diagnoses: NA\n",
            "13. • No past medical history (pmhx) mentioned\n",
            "14. Mobility Aids: NA\n",
            "15. Patient Monitoring & Safety Equipment: NA\n",
            "16. Bed Equipment: NA\n",
            "17. Specialized Medical Equipment: NA\n",
            "18. Assistance: NA\n",
            "19. IV: NA\n",
            "20. Pressure ulcers: NA\n",
            "21. Complex wounds: No\n",
            "22. Wound Vac: No\n",
            "23. Specialty Wound Equipment: No\n",
            "24. No\n",
            "25. NA\n",
            "26. NA\n",
            "27. Therapy: NA\n",
            "28. Concerns: NA\n",
            "29. Tuberculosis: No\n",
            "30. VRE: No\n",
            "31. HIV: No\n",
            "32. ESBL: No\n",
            "33. AIDS: No\n",
            "34. ECG: No\n",
            "35. PT/INR: No\n",
            "36. eCardio: No\n",
            "37. Bladder scan: No\n",
            "38. Central line: No\n",
            "39. Midline catheter: No\n",
            "40. Implantable ports: No\n",
            "41. JP/Penrose drain: No\n",
            "42. Nephrostomy: No\n",
            "43. Colostomy: No\n",
            "44. Suprapubic Catheter: No\n",
            "45. Dialysis Shunt: No\n",
            "46. Dialysis Catheter: No\n",
            "47. BiPAP/CPAP: No\n",
            "48. Tracheostomy: No\n",
            "49. Oxygen: No\n",
            "50. Portable/Astral vent: No\n",
            "51. Chest tube: No\n",
            "52. Pleurex: No\n",
            "53. Surgical Orthopedic care: No\n",
            "54. External fixator: No\n",
            "55. Orthopedic Traction: No\n",
            "56. Prosthetics: No\n",
            "57. CABG: No\n",
            "58. LVAD: No\n",
            "59. LifeVest: No\n",
            "60. Pacemaker: No\n",
            "61. Hemodialysis: No\n",
            "62. Peritoneal dialysis: No\n",
            "63. Bariatric: No\n",
            "64. IV: No\n",
            "65. TPN: No\n",
            "66. PCA: No\n",
            "67. IV Push: No\n",
            "68. Oral Chemotherapy: No\n",
            "69. IV Chemo: No\n",
            "70. High Cost Rx: No\n",
            "71. Psychosocial Care: No\n",
            "72. Dementia: No\n",
            "73. Recent danger: No\n",
            "74. Restraints: No\n",
            "75. Closed Unit: No\n",
            "76. Felon: No\n",
            "77. Sexual Offender: No\n",
            "78. Etoh/Alcohol-current: No\n",
            "79. Etoh/Alcohol-history: No\n",
            "80. Drugs-current: No\n",
            "81. Drugs-history: No\n",
            "82. Head Injury: No\n",
            "83. Less<18 yrs: No\n",
            "84. NA\n"
          ]
        }
      ],
      "source": [
        "# GPT_4 Model\n",
        "\n",
        "# import openai\n",
        "# from google.colab import userdata\n",
        "# import pandas as pd\n",
        "\n",
        "# # Load the data\n",
        "# df = pd.read_excel('./ChatGPT prompts v5.xlsx', sheet_name=1)\n",
        "# questions = df['Rafi\\'s Qs'].tolist()\n",
        "\n",
        "# # Read the file content\n",
        "# with open('/content/merged_deid1/merged_deidentified_text.txt', 'r') as file:\n",
        "#     content = file.read()\n",
        "\n",
        "# # Convert the list into a string with each question numbered\n",
        "# questions_str = '\\n'.join([f'{i+1}. {q}' for i, q in enumerate(questions)])\n",
        "\n",
        "# # Concatenate the strings\n",
        "# prompt = content + '\\n Based on the above patient\\'s medical record. Answer the following questions? \\n' + questions_str\n",
        "\n",
        "# # print(prompt)\n",
        "\n",
        "\n",
        "# # Ensure your OpenAI API key is correctly set here or through environment variables\n",
        "# openai.api_key = userdata.get('GPT_API')\n",
        "\n",
        "# def get_gpt3_5_turbo_response(prompt):\n",
        "#     try:\n",
        "#         # Construct the messages parameter correctly as a list of dictionaries\n",
        "#         messages = [\n",
        "#             {\"role\": \"system\", \"content\": \"You are a helpful medical record retriever.\"},\n",
        "#             {\"role\": \"user\", \"content\": prompt}\n",
        "#         ]\n",
        "\n",
        "#         response = openai.ChatCompletion.create(\n",
        "#             model=\"gpt-3.5-turbo-0125\",\n",
        "#             # model = \"gpt-4-0125-preview\",\n",
        "#             messages=messages,\n",
        "#             # max_tokens=4096 ,  # Adjust based on the expected length of completion, ensuring total does not exceed 16385\n",
        "#             # temperature=0.7,\n",
        "#             # top_p=1,\n",
        "#             # frequency_penalty=0,\n",
        "#             # presence_penalty=0\n",
        "#         )\n",
        "\n",
        "#         # Assuming the response structure matches expected output\n",
        "#         return response.choices[0].message['content'].strip() if response.choices else \"No response\"\n",
        "#     except Exception as e:\n",
        "#         print(f\"An error occurred: {e}\")\n",
        "#         return None\n",
        "\n",
        "# # Example usage\n",
        "# # prompt = \"What is the capital of France?\"\n",
        "# response_text = get_gpt3_5_turbo_response(prompt)\n",
        "# print(response_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTXvk0E3Jbl_"
      },
      "outputs": [],
      "source": [
        "# End"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "128f3645ed094306a8d9b06f0e26316a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34b6f0823ed84351b67ea9652c3065c8",
            "max": 1417588465,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96b04859711847d09293bae57a5ee77a",
            "value": 1417588465
          }
        },
        "1c429dfb87954a7fa913b8abcca8e01a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b5a3ecc7a2c4256890e4ba5f7ed762c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_716dc9a4a9c6407cab1f9df3a80345cc",
            "placeholder": "​",
            "style": "IPY_MODEL_c8a28669b59b449491c7001f862b04d8",
            "value": " 2.50k/2.50k [00:00&lt;00:00, 187kB/s]"
          }
        },
        "2f0b5807e6e84f8886c0a7cc61a71a1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "304f3d1be9e34a76b1c1f00d7c303b24": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34b6f0823ed84351b67ea9652c3065c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34c289e56b52439a936077023f137a88": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_911b69f5ad2b4e89a18c0db59083c120",
            "placeholder": "​",
            "style": "IPY_MODEL_b55747b61eba49ff96e777f9428f755d",
            "value": "config.json: 100%"
          }
        },
        "4fcc8aae02fc47e4bed197fdd88e77b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "642d2130631348a4ac0a23c7593c5f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f37c628088014aecb0cd21ca8db97539",
              "IPY_MODEL_128f3645ed094306a8d9b06f0e26316a",
              "IPY_MODEL_afa8c7f8772c4be2bc8127502c639016"
            ],
            "layout": "IPY_MODEL_304f3d1be9e34a76b1c1f00d7c303b24"
          }
        },
        "683de6500e604034960a10e88347e7f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "716dc9a4a9c6407cab1f9df3a80345cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "911b69f5ad2b4e89a18c0db59083c120": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93450af3eaff4829b6007c37dd9432d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_34c289e56b52439a936077023f137a88",
              "IPY_MODEL_c5a8487a2c4641b7bf95befc59c5f7e5",
              "IPY_MODEL_2b5a3ecc7a2c4256890e4ba5f7ed762c"
            ],
            "layout": "IPY_MODEL_963c0544c5204778aa988c53ca2cb062"
          }
        },
        "963c0544c5204778aa988c53ca2cb062": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96b04859711847d09293bae57a5ee77a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "afa8c7f8772c4be2bc8127502c639016": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f0b5807e6e84f8886c0a7cc61a71a1b",
            "placeholder": "​",
            "style": "IPY_MODEL_f8ae1995a01646998c88de0795f53ae5",
            "value": " 1.42G/1.42G [00:26&lt;00:00, 55.9MB/s]"
          }
        },
        "b55747b61eba49ff96e777f9428f755d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5a8487a2c4641b7bf95befc59c5f7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_683de6500e604034960a10e88347e7f8",
            "max": 2497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c429dfb87954a7fa913b8abcca8e01a",
            "value": 2497
          }
        },
        "c8a28669b59b449491c7001f862b04d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d006fa6d3e074ef2a4caaeb6d44c1f0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f37c628088014aecb0cd21ca8db97539": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fcc8aae02fc47e4bed197fdd88e77b1",
            "placeholder": "​",
            "style": "IPY_MODEL_d006fa6d3e074ef2a4caaeb6d44c1f0d",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "f8ae1995a01646998c88de0795f53ae5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}